{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Story Generator"
   ],
   "metadata": {
    "id": "b7077488"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Ready"
   ],
   "metadata": {
    "id": "60b8dfc3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 기본 패키지 임포트"
   ],
   "metadata": {
    "id": "28a05a6e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import GPT2LMHeadModel"
   ],
   "outputs": [],
   "metadata": {
    "id": "a4c51c98"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 학습 모델 다운로드"
   ],
   "metadata": {
    "id": "cf126c75"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "MODEL_NAME = \"skt/kogpt2-base-v2\""
   ],
   "outputs": [],
   "metadata": {
    "id": "35e0ec56"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 토크나이저 load"
   ],
   "metadata": {
    "id": "57aa65ec"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "TOKENS_DICT = {\n",
    "    'bos_token':'<s>',\n",
    "    'eos_token':'</s>',\n",
    "    'unk_token':'<unk>',\n",
    "    'pad_token':'<pad>',\n",
    "    'mask_token':'<mask>'\n",
    "}\n",
    "\n",
    "# 특수 토큰이 토크나이저에 추가되고 모델은 수정된 토크나이저에 맞게 임베딩의 크기를 조정\n",
    "tokenizer.add_special_tokens(TOKENS_DICT)\n",
    "\n",
    "print(tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fffd83bb",
    "outputId": "8ab92b3b-7fe0-4e96-e6c8-50fc59e0ff73"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 미세조정학습 모델 load"
   ],
   "metadata": {
    "id": "424e1abc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from util.generator import sample_sequence as gen\n",
    "from util.model import model_loading as load"
   ],
   "outputs": [],
   "metadata": {
    "id": "175de497"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "checkpointPath = \"./model/tale_model.tar\"\n",
    "loading = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model,ckp = load(checkpointPath, PU = 'cpu', status = loading)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(ckp['epoch'], ckp['loss'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2000 tensor(0.0307, requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "vocab_path = \"./data/정제/index_tale_plus_novel.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "context = \\\n",
    "\"\"\"\n",
    "이른 아침\n",
    "\"\"\"\n",
    "\n",
    "generated = gen(\n",
    "    vocab=vocab_path,\n",
    "    model=model, \n",
    "    length=500, \n",
    "    context=context,\n",
    "    num_samples=1, \n",
    "    repetition_penalty=2.0,\n",
    "    top_p=0.9,\n",
    "    tokenizer = tokenizer)\n",
    "print(tokenizer.decode(generated[0]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  9%|▉         | 18/200 [00:00<00:09, 18.76it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 11%|█         | 22/200 [00:01<00:13, 12.91it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스킵된 token >>  </s>\n",
      "그냥 넘어갑니다\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 13%|█▎        | 26/200 [00:01<00:13, 13.11it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스킵된 token >>  </s>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 83%|████████▎ | 166/200 [00:20<00:05,  5.67it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 84%|████████▎ | 167/200 [00:21<00:14,  2.29it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스킵된 token >>  </s>\n",
      "그냥 넘어갑니다\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 98%|█████████▊| 196/200 [00:29<00:01,  2.59it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n",
      "스킵된 token >>  </s>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 98%|█████████▊| 197/200 [00:31<00:02,  1.20it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "스킵된 token >>  </s>\n",
      "그냥 넘어갑니다\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "201it [00:32,  6.13it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word not in corpus => 16 skipped\n",
      "\n",
      "이른 아침\n",
      ".\n",
      " 서산마루에 차려질 땐 몰랐던 가족의 감정은 한꺼번에 터져 나왔습니다.\n",
      "아니다.</s> 버스를 찍어다오.\"\n",
      "넉볕을 믿고 여는지 떠들었습니다.\n",
      "오냐나 애벌레는 여전히 빙그레 웃기만 하고 날씨가 불어죽게 굴렀습니다.\n",
      "들야, 에다가 파도에 휩쓸리면 어떡하니? 머리가 잔뜩 때가 좋다, 카레 곳으로 가져왔어.\n",
      " 입가에 묻어 있는 것조차 마땅찮지가 않네, 껍데기도 해에도 봉숭아 뿌리가 깊은 바다까지.\" 손이 알려지고 후추 가는 것도 아주 덩달아 썩어가는 몇 개 받았다. 그래 본 적이 없으니까칠 동안 하루 종유. 그러고.\" \n",
      "\n",
      "\n",
      "수머리 없는 것처럼 얼굴이 나타났다.\n",
      " 그것들을 들여다보니 어디서 일을 아직은 털어버린왕비뼈만 날려보았다. 가지 않는 세상에 구멍이 있다고 생각했었다.</s>를시니더욱 사랑했다.\n",
      "바다 같은 것을 뻔히 보고 샘물에 걸려오는 고이지 한수는 내 마음을 저도 그만두려고 했다.\n",
      "\n",
      ".</s> 함께 조바닷\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "story generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "c84ec5531762457a514972294189478bb2b86b2856b4d2c66c8829aed73cefe4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}